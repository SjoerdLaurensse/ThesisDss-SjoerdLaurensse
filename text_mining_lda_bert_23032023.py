# -*- coding: utf-8 -*-
"""Text_mining_lda_bert_23032023

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XVf8GnBQXaDYwK6v_HSM9JkwvLUyl0RJ

# Install packages
"""

!pip install numpy pandas umap-learn hdbscan bertopic gensim contractions

!pip install sentence-transformers

"""# Load libraries"""

import json
import pandas as pd
import umap
import numpy as np
import matplotlib.pyplot as plt
import matplotlib as mpl
import re
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from transformers import DistilBertTokenizer, TFDistilBertModel
from bertopic import BERTopic
import hdbscan
from hdbscan import HDBSCAN
from umap import UMAP
from tqdm.auto import tqdm
from gensim.corpora import Dictionary
from gensim.models import CoherenceModel
from gensim.utils import simple_preprocess
from sklearn.datasets import fetch_20newsgroups
from sklearn.metrics import silhouette_score
from sklearn.feature_extraction.text import CountVectorizer
from sentence_transformers import SentenceTransformer
from itertools import product
from wordcloud import WordCloud
from nltk import FreqDist
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger')
import contractions

import numpy as np
from umap import UMAP
from bertopic import BERTopic
from sklearn.decomposition import PCA
from sklearn.feature_extraction.text import CountVectorizer
from bertopic.representation import KeyBERTInspired

from google.colab import drive
drive.mount('/content/drive')

"""# Loading headlines"""

# Load datasets with dataparsing so that both date columns are in the same format
NYT = pd.read_csv('/content/drive/MyDrive/Thesis Data Science Python/NYT_data.csv')
Waxy = pd.read_csv('/content/drive/MyDrive/Thesis Data Science Python/Waxy_data.csv')

# Renaming the column names in the same format and keeping only the columns of interest
Waxy = Waxy.rename(columns={'publish_date': 'date', 'headline_text': 'headline', 'headline_category': 'headline_category'})
Waxy = Waxy[['date', 'headline', 'headline_category']]

NYT = NYT.rename(columns={'headline': 'headline', 'date': 'date', 'doc_type': 'headline_category'})
NYT = NYT[['date', 'headline', 'headline_category']]


# Adding colum for source
Waxy['source'] = 'Waxy'
NYT['source'] = 'New York Times'


Waxy['date'] = pd.to_datetime(Waxy['date'], format = '%Y%m%d')

# Concatenate the preprocessed datasets into one dataset
merged_df = pd.concat([Waxy, NYT], ignore_index=True)

# Convert the date column to a datetime object and sort ascendingly
merged_df['date'] = pd.to_datetime(merged_df['date'])
merged_df = merged_df.sort_values(by='date').reset_index(drop=True)

# date index
merged_df = merged_df.set_index('date')
merged_df = merged_df.sort_values(by = 'date', ascending=True)
df = merged_df.copy()

# Filter
start_date = '2009-01-01'
end_date = '2017-01-01'

df = df.loc[start_date:end_date]

# Convert to right format
df['headline'] = df['headline'].astype(str)
df['headline_category'] = df['headline_category'].astype(str)
df['source'] = df['source'].astype(str)

"""# Text Preprocessing"""

# removing url, HTML tags, non-ASCII characters, and other special characters

def remove_noise(text):
    """
    Remove URLs, HTML tags, non-ASCII characters, and special characters from text.
    """
    url_pattern = re.compile(r"https?://\S+|www\.\S+")
    html_pattern = re.compile(r"<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});")
    special_pattern = re.compile(r'[^\w\s]|_')  # removes special characters except whitespace
    emoji_pattern = re.compile("["
        u"\U0001F600-\U0001F64F"  # emoticons
        u"\U0001F300-\U0001F5FF"  # symbols & pictographs
        u"\U0001F680-\U0001F6FF"  # transport & map symbols
        u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
                           "]+")

    # remove URLs
    text = re.sub(url_pattern, "", text)
    # remove HTML tags
    text = re.sub(html_pattern, "", text)
    # remove non-ASCII characters
    text = text.encode('ascii', 'ignore').decode('utf-8')
    # remove special characters except whitespace
    text = re.sub(special_pattern, "", text)
    # remove emojis
    text = emoji_pattern.sub("", text)

    return text.strip()


# getting rid of punctuation
def remove_punctuation(text):

    return re.sub(r'[]!"$%&\'()*+,./:;=#@?[\\^_`{|}~-]+', "", text)


# Define the function to stem the corpus. Stemming is done to identify the common stem among different versions of forms of a word. More specifically, it is extracting the root.

def snowball_stemmer(text):
    stemmer = nltk.SnowballStemmer("english")
    stems = [stemmer.stem(i) for i in text]
    return stems


# Define a function to lemmatize words. This means determining that two words have the same root, though they 'look' different.
def lemmatize_word(text):
    lemmatizer = WordNetLemmatizer()
    lemma = [lemmatizer.lemmatize(word, tag) for word, tag in text]
    return lemma

lemmatizer = WordNetLemmatizer()

# First, text is cleaned
# lower case
df["headline_clean"] = df["headline"].apply(lambda x: x.lower())

# contractions
df["headline_clean"] = df["headline_clean"].apply(lambda x: contractions.fix(x))

# noise removal function
df['headline_clean'] = df['headline_clean'].apply(lambda x: remove_noise(x))

# puctuation removal
df["headline_clean"] = df["headline_clean"].apply(lambda x: remove_punctuation(x))

df['headline_tok'] = df['headline_clean'].apply(word_tokenize)

# Removing the stopwords from the corpus
# It should be noted that removing the stop words that needs careful experimentation depending on the context of the research. Mostly, it depends on the length of possible bigrams you would want to make and the possibility of missing information
stop = set(stopwords.words('english'))
df['headline_tok'] = df['headline_tok'].apply(lambda x: [word for word in x if word not in stop])

# Stemming the corpus with pre-defined function
df['headline_stem'] = df['headline_tok'].apply(lambda x: snowball_stemmer(x))

df['headline_lemma'] = df['headline_tok'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])

import pandas as pd
import numpy as np

filter_words = ['irish', 'ireland', 'new', 'york']

keywords_subset2 = ['natural', 'gas', 'warm',
                    'crude', 'oil', 'supply', 'dollar', 'euro',
                    'iran', 'russia', 'china', 'mexico',
                    'stocks', 'trade', 'composite', 'dax', 's&p', 'dow', 'jones', 'industrial', 'shares']

keywords_subset3 = ['nymex', 'oil', 'wti', 'crude', 'price']

keywords_subset4 = ['crude', 'oil', 'crude oil']

# create a boolean mask for rows where headline_lemma contains any of the keywords
mask_include_subset2 = np.column_stack([df['headline_clean'].str.contains(r"\b{}\b".format(keyword), na=False, case=False) for keyword in keywords_subset2]).any(axis=1)
mask_include_subset3 = np.column_stack([df['headline_clean'].str.contains(r"\b{}\b".format(keyword), na=False, case=False) for keyword in keywords_subset3]).any(axis=1)
mask_include_subset4 = np.column_stack([df['headline_clean'].str.contains(r"\b{}\b".format(keyword), na=False, case=False) for keyword in keywords_subset4]).any(axis=1)

# create a boolean mask for rows where headline_lemma contains any of the filter_words
mask_exclude = np.column_stack([df['headline_clean'].str.contains(r"\b{}\b".format(word), na=False, case=False) for word in filter_words]).any(axis=1)

# filter the dataframe for multiple subsets
subset1 = df[~mask_exclude]
subset2 = df[mask_include_subset2 & ~mask_exclude]
subset3 = df[mask_include_subset3 & ~mask_exclude]
subset4 = df[mask_include_subset4 & ~mask_exclude]

import matplotlib.pyplot as plt
from wordcloud import WordCloud

# We will join each list of headlines into a single string for each dataset
text1 = ' '.join(subset1['headline_clean'])
text2 = ' '.join(subset2['headline_clean'])

wordcloud1 = WordCloud(max_font_size=50, max_words=100, background_color="white").generate(text1)
wordcloud2 = WordCloud(max_font_size=50, max_words=100, background_color="white").generate(text2)


mpl.rcParams.update(mpl.rcParamsDefault)
mpl.rcParams['font.family'] = 'STIXGeneral'

# Create a figure and a grid of subplots
fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(10, 10))

# Plot word cloud on each subplot
ax1.imshow(wordcloud1, interpolation='bilinear')
ax1.axis("off")
ax1.set_title('Before Filtering', fontsize = 15)

ax2.imshow(wordcloud2, interpolation='bilinear')
ax2.axis("off")
ax2.set_title('After Filtering', fontsize = 15)

# Save the figure
plt.savefig('/content/drive/MyDrive/Thesis Data Science Python/wordclouds.jpg', format='jpg', dpi=300, bbox_inches='tight', pad_inches = 0)

# Show the figure
plt.show()

"""# Get train data in place"""

def prepare_train(dataset, start_date = '2009-01-01', end_date = '2017-01-01'):
    dataset = dataset.dropna(subset = ['headline_clean'])
    dataset['headline_clean'] = dataset['headline_clean'].astype(str)
    dataset = dataset.dropna(subset = ['headline_lemma'])
    dataset = dataset[start_date:end_date]
    return dataset

# multiple subset for experimental phase
subset1 = prepare_train(subset1)
subset2 = prepare_train(subset2)
subset3 = prepare_train(subset3)
subset4 = prepare_train(subset4)

"""# Apply TF-IDF

# Apply BERTopic
"""

def bertopic_model(dataset, output_file, model_file):
    # Load dataset as list
    data = dataset['headline_clean'].tolist()

    # prepare embeddings with setnence transformer
    docs = dataset['headline_clean']
    sentence_model = SentenceTransformer("all-MiniLM-L6-v2")
    embeddings = sentence_model.encode(docs, show_progress_bar=True)

    # Preprocess data for coherence calculation
    tokenized_data = [simple_preprocess(doc) for doc in data]
    dictionary = Dictionary(tokenized_data)
    corpus = [dictionary.doc2bow(doc) for doc in tokenized_data]

    # define vectorizer model
    vectorizer_model = CountVectorizer(stop_words="english")

    # Initialize UMAP and HDBSCAN models with optimized parameters
    umap_model = UMAP(metric="cosine", n_neighbors = 5, low_memory=True)
    hdbscan_model = HDBSCAN(metric='euclidean', min_cluster_size = 25, prediction_data=True)


    topic_model = BERTopic(umap_model=umap_model,
                            hdbscan_model=hdbscan_model,
                            nr_topics='auto',
                            vectorizer_model=vectorizer_model,
                            min_topic_size = 20,
                            n_gram_range=(1, 2))

    # Fit the model
    topics, probabilities = topic_model.fit_transform(docs, embeddings)

    # Save the BERTopic model
    topic_model.save(model_file)

    # Use the 'distribution' strategy to assign outliers to topics
    new_topics_dis = topic_model.reduce_outliers(docs, topics, strategy="distributions")

    topic_model.update_topics(docs, topics=new_topics_dis)

    dataset['Updated_topics'] = new_topics_dis

    # Create a pivot table with all the values from df
    pivot_table = dataset.pivot_table(index=dataset.index, columns='Updated_topics', aggfunc='size', fill_value=0)

    # Normalize the counts by dividing each cell by the total count of documents for that day
    normalized_table_updated = pivot_table.div(pivot_table.sum(axis=1), axis=0)
    normalized_table_updated.fillna(0, inplace=True)


    # replace the fixed filename with the output_file parameter
    normalized_table_updated.to_csv(output_file)

    return topic_model, normalized_table_updated



# Now apply the topic model
topic_model_subset3_bert, norm_table_subset3_bert = bertopic_model(subset2, '/content/drive/MyDrive/Thesis Data Science Python/topic_time_series_train_v6_bert_subset3.csv', '/content/drive/MyDrive/Thesis Data Science Python/topic_model_bert_subset3')

def get_bertopic_representations(topic_model):
    topic_info = topic_model.get_topic_info()

    # Sort topics by frequency
    topic_info_sorted = topic_info.sort_values(by='Count', ascending=False)

    for index, row in topic_info_sorted.iterrows():
        print(f"Topic {row['Topic']}, Frequency: {row['Count']}, Representation: {row['Representation']}")

# Apply the function to print the topics
get_bertopic_representations(topic_model_subset3_bert)

import gensim
from sklearn.feature_extraction.text import CountVectorizer
from gensim import corpora, models

def lda_model(dataset, output_file, model_file):
    # load dataset as list of lists
    data = dataset['headline_lemma'].tolist()

    # Create a dictionary representation
    dictionary = corpora.Dictionary(data)

    # vectorize the data using the dictionary
    corpus = [dictionary.doc2bow(doc) for doc in data]

    # LDA model on corpus and save the model for further processing
    ldamodel = gensim.models.LdaModel(corpus, id2word=dictionary, num_topics=33, passes=25, random_state=34)

    # save the LDA model
    ldamodel.save(model_file)

    # assign topics back to the dataframe
    topics = [max(ldamodel.get_document_topics(bow), key=lambda x:x[1])[0] for bow in corpus]
    dataset['Updated_topics'] = topics

    #create a pivot table with all the values from df
    pivot_table = dataset.pivot_table(index=dataset.index, columns='Updated_topics', aggfunc='size', fill_value=0)

    # normalize the counts by dividing each cell by the total count of documents for that day
    normalized_table_updated = pivot_table.div(pivot_table.sum(axis=1), axis=0)

    # fill NA with 0
    normalized_table_updated.fillna(0, inplace=True)

    # Save the normalized table to the specified output file
    normalized_table_updated.to_csv(output_file)

    return ldamodel, normalized_table_updated

# now apply the topic model
topic_model_subset3_lda, norm_table_subset3_lda = lda_model(subset2, '/content/drive/MyDrive/Thesis Data Science Python/topic_time_series_train_v6_lda_subset3.csv', '/content/drive/MyDrive/Thesis Data Science Python/topic_model_lda_subset3')

from collections import Counter

# get dominant topic for each document
dominant_topics = [max(ldamodel.get_document_topics(bow), key=lambda x:x[1])[0] for bow in corpus]

# count the occurrence of each topic
topic_counts = Counter(dominant_topics)

def print_topics(ldamodel, num_topics, num_words=10):
    for idx, topic in ldamodel.show_topics(formatted=False, num_topics=num_topics, num_words=num_words):
        print(f'Topic: {idx}, Count: {topic_counts[idx]}\nWords: {"|".join([w for w, _ in topic])}')

# apply the function to print the topics
print_topics(topic_model_subset3_lda, num_topics=32)

# Create a dictionary representation of the documents
dictionary_subset3_lda = corpora.Dictionary(subset2['headline_lemma'].tolist())

# Vectorize the data using the dictionary
corpus_subset3_lda = [dictionary_subset3_lda.doc2bow(doc) for doc in subset2['headline_lemma'].tolist()]

def print_topics(ldamodel, corpus, num_topics, num_words=3):
    dominant_topics = [max(ldamodel.get_document_topics(bow), key=lambda x:x[1])[0] for bow in corpus]
    topic_counts = Counter(dominant_topics)

    for idx, topic in ldamodel.show_topics(formatted=False, num_topics=num_topics, num_words=num_words):
        print(f'Topic: {idx}, Count: {topic_counts[idx]}\nWords: {"|".join([w for w, _ in topic])}')


# Apply the function to print the topics
print_topics(topic_model_subset3_lda, corpus_subset3_lda, num_topics=33)

def print_topics(ldamodel, num_topics, num_words=10):
    for idx, topic in ldamodel.show_topics(formatted=False, num_topics=num_topics, num_words=num_words):
        print('Topic: {} \nWords: {}'.format(idx, '|'.join([w for w, _ in topic])))

# Apply the function to print the topics
print_topics(topic_model_subset3_lda, num_topics=32)

def aggregate_weekends(input_file, output_file):
    df = input_file
    df.index = pd.to_datetime(df.index)

    # create new index
    weekday_map = df.index.to_series().apply(lambda x: x - pd.DateOffset(days=x.weekday() - 4) if x.weekday() > 4 else x)
    df.index = weekday_map
    df = df.groupby(df.index).mean()
    df = df[df.index.weekday < 5]

    df.to_csv(output_file)
    print(df)

    return df

aggregate_weekends(norm_table_subset3_lda, '/content/drive/MyDrive/Thesis Data Science Python/topic_time_series_train_v6_lda_subset3.csv')
aggregate_weekends(norm_table_subset3_bert, '/content/drive/MyDrive/Thesis Data Science Python/topic_time_series_train_v6_bert_subset3.csv')