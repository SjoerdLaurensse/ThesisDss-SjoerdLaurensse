# -*- coding: utf-8 -*-
"""Forecasting oil prices / 23032023

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PzU980rxme3YOUEH5ye-itgWwf9eV-8P

# Install libraries
"""

!pip install umap-learn
!pip install hdbscan
!pip install bertopic
!pip install huggingface_hub
!pip install keras-tuner
!pip install pandas-profiling[notebook]
!pip install shap

"""# Import packages"""

import os
import math
import datetime as dt

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import statsmodels.api as sm
from scipy import stats

# Statsmodels specific imports
from statsmodels.tsa.api import VAR
from statsmodels.tsa.arima.model import ARIMA
from statsmodels.tsa.statespace.sarimax import SARIMAX
from statsmodels.tsa.stattools import adfuller
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
from statsmodels.tools.eval_measures import rmse, aic

# Sklearn specific imports
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.model_selection import train_test_split, TimeSeriesSplit
from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import SVR
from sklearn.feature_selection import RFE
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import RandomizedSearchCV
from sklearn.metrics import mean_absolute_error, mean_squared_error
from sklearn.model_selection import TimeSeriesSplit
import numpy as np

import pickle

# Keras and Tensorflow specific imports
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.optimizers import Adam
from kerastuner.tuners import RandomSearch
from tensorflow.keras.losses import MeanAbsoluteError, MeanSquaredError
from keras_tuner import Hyperband


# NLP specific imports
import nltk
import re
from nltk.corpus import stopwords
from nltk.stem import SnowballStemmer, WordNetLemmatizer
from nltk.tokenize import word_tokenize
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
nltk.download('stopwords')
nltk.download('wordnet')

# Other specific imports
from bertopic import BERTopic
from bertopic.representation import MaximalMarginalRelevance
from umap import UMAP
from hdbscan import HDBSCAN
from sentence_transformers import SentenceTransformer

import gensim
from gensim import corpora
import numpy as np
import pandas as pd
from scipy.stats import ttest_rel
from scipy.stats import ttest_1samp


import warnings
from math import sqrt


import seaborn as sns
from statsmodels.graphics.gofplots import qqplot

# Importing additional library
from matplotlib.ticker import EngFormatter

import matplotlib.pyplot as plt
import matplotlib.font_manager as fm

from google.colab import drive
drive.mount('/content/drive')

"""# Predefine the train - val - test split

# Applying BERTopic
"""

def load_and_preprocess_dataset(file_path):
    df = pd.read_csv(file_path)

    # Convert data types to right format
    df['headline'] = df['headline'].astype(str)
    df['headline_category'] = df['headline_category'].astype(str)
    df['source'] = df['source'].astype(str)

    # Convert the 'date' column to datetime objects
    df['date'] = pd.to_datetime(df['date'])

    # Filter the dataset based on the date range
    df = df[(df['date'] >= '2017-01-01') & (df['date'] < '2020-01-01')]

    return df


# Removing noise from text
def remove_noise(text):
    url_pattern = re.compile(r"https?://\S+|www\.\S+")
    html_pattern = re.compile(r"<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});")
    special_pattern = re.compile(r'[^\w\s]|_')  # removes special characters except whitespace
    emoji_pattern = re.compile("["
        u"\U0001F600-\U0001F64F"  # emoticons
        u"\U0001F300-\U0001F5FF"  # symbols & pictographs
        u"\U0001F680-\U0001F6FF"  # transport & map symbols
        u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
                           "]+")

    # remove URLs
    text = re.sub(url_pattern, "", text)
    # remove HTML tags
    text = re.sub(html_pattern, "", text)
    # remove non-ASCII characters
    text = text.encode('ascii', 'ignore').decode('utf-8')
    # remove special characters except whitespace
    text = re.sub(special_pattern, "", text)
    # remove emojis
    text = emoji_pattern.sub("", text)

    return text.strip()


# To get rid of all punctions, I will construct a function to remove it

def remove_punctuation(text):
    return re.sub(r'[]!"$%&\'()*+,./:;=#@?[\\^_`{|}~-]+', "", text)

    # Create a function to check if any keyword is present in the headline



def clean_and_preprocess_text(df, text_col_name):
    lemmatizer = WordNetLemmatizer()
    stop = set(stopwords.words('english'))

    # Text is cleaned
    df[text_col_name+"_clean"] = df[text_col_name].apply(lambda x: x.lower())

    # Remove noise and punctuation
    df[text_col_name+'_clean'] = df[text_col_name+'_clean'].apply(lambda x: remove_noise(x))
    df[text_col_name+"_clean"] = df[text_col_name+"_clean"].apply(lambda x: remove_punctuation(x))

    # Tokenize words and remove stopwords
    df['headline_preprocessed'] = df[text_col_name+"_clean"].apply(word_tokenize)
    df['headline_preprocessed'] = df['headline_preprocessed'].apply(lambda x: [word for word in x if word not in stop])

    # Lemmatization
    df['headline_lemma'] = df['headline_preprocessed'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])


    # Remove missing velues
    df = df.dropna(subset=['headline_clean'])
    df['headline_clean'] = df['headline_clean'].astype(str)
    df['headline_lemma'] = df['headline_lemma'].astype(str)


    # Set index of date
    df.set_index('date', inplace = True)

    return df


test_subset2 = load_and_preprocess_dataset('/content/drive/MyDrive/Thesis Data Science Python/full_subset2.csv')


test_subset2 = clean_and_preprocess_text(test_subset2, 'headline')

def prepare_test(dataset, start_date = '2017-01-01', end_date = '2020-01-01'):
    dataset = dataset.dropna(subset = ['headline_clean'])
    dataset['headline_clean'] = dataset['headline_clean'].astype(str)
    dataset = dataset.dropna(subset = ['headline_lemma'])
    dataset['headline_lemma'] = dataset['headline_lemma'].astype(str)
    dataset = dataset[start_date:end_date]
    return dataset

test_topic_subset2 = prepare_test(test_subset2)

def aggregate_weekends(df):
    # Make sure your DataFrame's index is of `datetime` type
    df.index = pd.to_datetime(df.index)

    # Create a new index that maps Saturday and Sunday to the preceding Friday
    weekday_map = df.index.to_series().apply(lambda x: x - pd.DateOffset(days=x.weekday() - 4) if x.weekday() > 4 else x)

    # Set this new index in your DataFrame
    df.index = weekday_map

    # Group by this new index and take the mean of the topic distributions
    df = df.groupby(df.index).mean()

    # Now, remove the original Saturday and Sunday dates from your DataFrame
    df = df[df.index.weekday < 5]

    return df

def bertopic(df, headline_column, topic_path):
    docs = df[headline_column].tolist()

    topic_model = BERTopic.load(topic_path)

    # Prepare embeddings for model - should be similar to the one used before
    sentence_model = SentenceTransformer("all-MiniLM-L6-v2")
    embeddings = sentence_model.encode(docs, show_progress_bar=True)
    topics, _ = topic_model.transform(docs, embeddings)

    # Reduce outliers using the `distributions` strategy
    new_topics = topic_model.reduce_outliers(docs, topics, strategy="distributions")

    topic_model.update_topics(docs, topics=new_topics)

    df['Updated_topics'] = new_topics

    # Create a pivot table for further processing
    pivot_table = df.pivot_table(index=df.index, columns='Updated_topics', aggfunc='size', fill_value=0)

    # Normalize the counts by dividing each cell by the total count of documents for that day and fill na
    normalized_table_updated = pivot_table.div(pivot_table.sum(axis=1), axis=0)
    normalized_table_updated.fillna(0, inplace=True)
    df = normalized_table_updated

    df.index = pd.to_datetime(df.index)

    return df



def ldatopic(df, headline_column, topic_path):

    # Load dataset as list of lists
    data = df[headline_column].str.split().tolist()

    # Load model
    lda_model = gensim.models.LdaModel.load(topic_path)

    # Create a dictionary representation of the documents
    dictionary = corpora.Dictionary(data)

    # Preprocess and vectorize the data
    corpus = [dictionary.doc2bow(doc) for doc in data]

    # Assign topics
    topics = [max(lda_model.get_document_topics(bow), key=lambda x:x[1])[0] for bow in corpus]

    # Then inlcude them in the dataset
    df['Updated_topics'] = topics

    # Create a pivot table with all the values from df
    pivot_table = df.pivot_table(index=df.index, columns='Updated_topics', aggfunc='size', fill_value=0)

    # Normalize the counts by dividing each cell by the total count of documents for that day
    normalized_table_updated = pivot_table.div(pivot_table.sum(axis=1), axis=0)

    # fill NA with 0
    normalized_table_updated.fillna(0, inplace=True)

    df = normalized_table_updated

    df.index = pd.to_datetime(df.index)

    return df



lda_subset2 = ldatopic(test_topic_subset2, 'headline_lemma', "/content/drive/MyDrive/Thesis Data Science Python/topic_model_lda_subset3")
bert_subset2 = bertopic(test_topic_subset2, 'headline_clean', "/content/drive/MyDrive/Thesis Data Science Python/topic_model_bert_subset3")


lda_subset2 = aggregate_weekends(lda_subset2)

bert_subset2 = aggregate_weekends(bert_subset2)

bert_subset2.to_csv('/content/drive/MyDrive/Thesis Data Science Python/test_bert_subset3.csv')
lda_subset2.to_csv('/content/drive/MyDrive/Thesis Data Science Python/test_lda_subset3.csv')

"""## Merge the train and test set for the topics together"""

# Make function to normlize the column that is specified for the dataframe that is specified
def normalize_column(df, column_name):
    df_copy = df.copy()
    scaler = MinMaxScaler()

    # data should be reshaped
    data = df_copy[column_name].values.reshape(-1, 1)


    normalized_data = scaler.fit_transform(data)

    # Add normalized data as a new column
    df_copy[column_name + '_norm'] = normalized_data
    return df_copy, scaler



def load_process_merge(topics_test_df_path, topics_train_df_path, price_df_path, start_date='2009-01-01', end_date='2020-01-01'):

    # Load train and test topic datasets
    topics_train = pd.read_csv(topics_train_df_path, parse_dates = ['date'])
    topics_test = pd.read_csv(topics_test_df_path, parse_dates = ['date'])

    # Convert dates to datetime and merge

    topics_train['date'] = pd.to_datetime(topics_train['date'])
    topics_train.set_index('date', inplace=False)


    topics_test['date'] = pd.to_datetime(topics_test['date'])
    topics_test.set_index('date', inplace=False)

    print(len(topics_test))



    #topics_test['date'] = pd.to_datetime(topics_test['date'])

    #topics_test.index = pd.to_datetime(topics_test.index)
    topics = pd.concat([topics_train, topics_test], axis=0)
    topics.set_index(['date'], inplace=True)

    # Load price data, convert datetime and merge with topics
    price_df = pd.read_csv(price_df_path)
    price_df['date'] = pd.to_datetime(price_df['date'])
    price_df.set_index('date', inplace=True)
    price_df = price_df.loc[start_date:end_date]



    # Merge the frames
    merged_df = price_df.merge(topics, how='left', left_index=True, right_index=True)

    # Clean up merged dataframe
    merged_df = merged_df.fillna(0)

    merged_df = merged_df.drop(['dji_price', 'usd_price', 'gold_price'],axis=1)

    # Filter date range
    full_df = merged_df.loc[start_date:end_date]

    new_df = full_df.copy()


    #normalize df
    new_df, scaler = normalize_column(new_df, 'oil_price')
    return new_df, topics, scaler


bert_subset2, topics, scaler = load_process_merge('/content/drive/MyDrive/Thesis Data Science Python/test_bert_subset3.csv', '/content/drive/MyDrive/Thesis Data Science Python/topic_time_series_train_v6_bert_subset3.csv','/content/drive/MyDrive/Thesis Data Science Python/merged_price_data.csv' )
lda_subset2, topics, scaler = load_process_merge('/content/drive/MyDrive/Thesis Data Science Python/test_lda_subset3.csv', '/content/drive/MyDrive/Thesis Data Science Python/topic_time_series_train_v6_lda_subset3.csv','/content/drive/MyDrive/Thesis Data Science Python/merged_price_data.csv' )

"""# Preprocessing

## Create supervised learning by lagging variables
## Assigning x and y
## Split train, val test
"""

def load_process_merge(price_df_path, start_date='2009-01-01', end_date='2020-01-01'):

    # Load price data, convert datetime and merge with topics
    price_df = pd.read_csv(price_df_path)
    price_df['date'] = pd.to_datetime(price_df['date'])
    price_df.set_index('date', inplace=True)

    # Merge the frames
    merged_df = price_df.merge(topics, how='left', left_index=True, right_index=True)

    # Clean up merged dataframe

    merged_df = merged_df.fillna(0)
    merged_df = merged_df[['oil_price']]
    # Filter date range
    full_df = merged_df.loc[start_date:end_date]

    new_df = full_df.copy()


    #normalize df
    new_df, scaler = normalize_column(new_df, 'oil_price')
    return new_df, scaler


base, scaler = load_process_merge('/content/drive/MyDrive/Thesis Data Science Python/merged_price_data.csv' )

# Define the minimum and maximum dates
min_date = pd.to_datetime('2009-01-01')
val_split = pd.to_datetime('2015-01-01')
test_split = pd.to_datetime('2017-01-01')
max_date = pd.to_datetime('2020-01-01')

# https://forecastegy.com/posts/time-series-cross-validation-python/

def process_data(full_dataset, test_split, val_split):

    # Create a new column for price movement
    #full_dataset['price_movement'] = full_dataset['oil_price_norm'].diff().abs()


    # Shift all independent variables one period with shift function
    full_dataset = full_dataset.shift(1)

    # Drop first row because dataframe was shifted
    full_dataset = full_dataset.iloc[1:]

    # Define X and y
    y = full_dataset['oil_price_norm'].shift(-1).to_frame()
    X = full_dataset.drop(['oil_price'], axis = 1)


    y = y.iloc[1:]
    X = X.iloc[1:]
    y = y.iloc[:-1]
    X = X.iloc[:-1]

    # Split the datasets into training and test sets while preserving the order
    X_train, X_test = X[:test_split], X[test_split:]
    y_train, y_test  = y[:test_split], y[test_split:]

    # Split the training datasets into training and validation sets while preserving the order
    X_train, X_val = X_train[:val_split], X_train[val_split:]
    y_train, y_val  = y_train[:val_split], y_train[val_split:]

    return X_train, X_val, X_test, y_train, y_val, y_test, full_dataset


# And 2
X_train_2b, X_val_2b, X_test_2b, y_train_2b, y_val_2b, y_test_2b, bert_subset2 = process_data(bert_subset2, test_split, val_split)
X_train_2l, X_val_2l, X_test_2l, y_train_2l, y_val_2l, y_test_2l, lda_subset2 = process_data(lda_subset2, test_split, val_split)

# Base set
X_train_base, X_val_base, X_test_base, y_train_base, y_val_base, y_test_base, base = process_data(base, test_split, val_split)

"""# Recursive feature elimination that returns new dataframe with optimal features"""

from sklearn.model_selection import TimeSeriesSplit
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error
from sklearn.feature_selection import RFE
import numpy as np

def rfe_cv_function(X_train, y_train, X_val, X_test, y_val, y_test, full_dataframe, tscv):
    # define rf
    rf = RandomForestRegressor(random_state=42)

    # make list for error rates
    error_rate = []

    # make variables to check optimal numbers
    min_error = float('inf')
    optimal_num_features = None
    optimal_features = None

    # Start with all features and remove one each time
    for num_features in range(1, X_train.shape[1], 1):
        rfe = RFE(rf, n_features_to_select=num_features, step=1)

        # Define the RFE object
        rfe = rfe.fit(X_train, y_train)

        # Names of most important features
        selected_features = [X_train.columns[i] for i in rfe.get_support(indices=True)]

        # List for error
        errors = []

        # Perform cross-validation
        for train_index, val_index in tscv.split(X_train[selected_features]):
            # Split the data
            X_train_cv, X_val_cv = X_train[selected_features].iloc[train_index], X_train[selected_features].iloc[val_index]
            y_train_cv, y_val_cv = y_train.iloc[train_index], y_train.iloc[val_index]

            # Train model with selected features for this loop and make predictions
            rf.fit(X_train_cv, y_train_cv)
            pred = rf.predict(X_val_cv)

            # compute error
            error = mean_squared_error(y_val_cv, pred, squared=False)
            errors.append(error)

        # Average error across folds
        avg_error = np.mean(errors)
        error_rate.append(avg_error)

        # Update the optimal number of features and error if the current error is smaller
        if avg_error < min_error:
            min_error = avg_error
            optimal_num_features = num_features
            optimal_features = selected_features

    # Get the feature importances for the optimal feature set
    rf_optimal = RandomForestRegressor(random_state=42)
    rf_optimal.fit(X_train[optimal_features], y_train)
    importances = rf_optimal.feature_importances_
    indices = np.argsort(importances)[::-1]
    features_ranked = [optimal_features[i] for i in indices]

    return (
        X_train[optimal_features],
        X_val[optimal_features],
        X_test[optimal_features],
        error_rate,
        min_error,
        full_dataframe[optimal_features],
        features_ranked,
        indices
    )

# Define the cross-validation object
tscv = TimeSeriesSplit(n_splits=5)


X_train_2b, X_val_2b, X_test_2b, error_rate_2b, min_error_2b, bert_subset2, features_ranked_bert, indices_bert = rfe_cv_function(X_train_2b, y_train_2b, X_val_2b, X_test_2b, y_val_2b, y_test_2b, bert_subset2, tscv)
X_train_2l, X_val_2l, X_test_2l, error_rate_2l, min_error_2l, lda_subset2, features_ranked_lda, indices_lda = rfe_cv_function(X_train_2l, y_train_2l, X_val_2l, X_test_2l, y_val_2l, y_test_2l, lda_subset2, tscv)

"""## Pickle"""



# Create a dictionary to store all the objects
data_to_save = {
    'X_train_2b': X_train_2b,
    'X_val_2b': X_val_2b,
    'X_test_2b': X_test_2b,
    'error_rate_2b': error_rate_2b,
    'min_error_2b': min_error_2b,
    'bert_subset2': bert_subset2,
    'features_ranked_bert': features_ranked_bert,
    'indices_bert': indices_bert,
    'X_train_2l': X_train_2l,
    'X_val_2l': X_val_2l,
    'X_test_2l': X_test_2l,
    'error_rate_2l': error_rate_2l,
    'min_error_2l': min_error_2l,
    'lda_subset2': lda_subset2,
    'features_ranked_lda': features_ranked_lda,
    'indices_lda': indices_lda
}

# Open a file for writing
with open('/content/drive/MyDrive/Thesis Data Science Python/oil_price_pickle.pkl', 'wb') as f:
    # Pickle the dictionary
    pickle.dump(data_to_save, f)

import pickle

with open('/content/drive/MyDrive/Thesis Data Science Python/oil_price_pickle.pkl', 'rb') as f:
    data_loaded = pickle.load(f)

# Now you can access the saved objects with their original names
X_train_2b = data_loaded['X_train_2b']
X_train_2l = data_loaded['X_train_2l']

X_val_2b = data_loaded['X_val_2b']
X_val_2l = data_loaded['X_val_2l']

X_test_2b = data_loaded['X_test_2b']
X_test_2l = data_loaded['X_test_2l']


# ... and so on for the other variables

import matplotlib.pyplot as plt
import matplotlib as mpl


# Set the size of the figure
fig, ax = plt.subplots(figsize=(10, 4))

# Plot the error rates
ax.plot(range(33), error_rate_2l, marker='o', linestyle='-', label='LDA model')
ax.plot(range(33), error_rate_2b, marker='o', linestyle='-', label='BERTopic model')


ax.set_title("Error Rate Versus Number of Features", fontsize=14, fontweight='bold')

ax.set_xlabel("Number of Features", fontsize=12)

ax.set_ylabel("Error Rate", fontsize=12)

ax.legend(fontsize='large')


plt.savefig('/content/drive/MyDrive/Thesis Data Science Python/RFE_results_combined.pdf', format='pdf')

plt.show()

print(f"Lowest error for the 2-layer model: {min_error_2l}")
print(f"Lowest error for the 2-branch model: {min_error_2b}")

"""# Descriptive Statistics

## Train, val, test against news headline count

## Heatmaps
"""

# Concat the train, val and test values
train_2b = pd.concat([X_train_2b, y_train_2b], axis = 1)
train_2l = pd.concat([X_train_2l, y_train_2l], axis = 1)
val_2b = pd.concat([X_val_2b, y_val_2b], axis = 1)
val_2l = pd.concat([X_val_2l, y_val_2l], axis = 1)
test_2b = pd.concat([X_test_2b, y_test_2b], axis = 1)
test_2l = pd.concat([X_test_2l, y_test_2l], axis = 1)


full_2b = pd.concat([X_train_2b, X_val_2b, X_test_2b], axis = 0)
full_2l = pd.concat([X_train_2l, X_val_2l, X_test_2l], axis = 0)

# Calculate Spearman's rank correlation
spearman_corr_BERT = full_2b.corr(method='spearman')
spearman_corr_LDA = full_2l.corr(method='spearman')

# Create a figure and a set of subplots
fig, (ax1, ax2, cbar_ax) = plt.subplots(1, 3, figsize=(16, 8), gridspec_kw={'width_ratios': [1, 1, 0.02]})  # 3 columns

# Generate a heatmap for BERT
sns.heatmap(spearman_corr_BERT,
            ax=ax1,
            annot=True,
            fmt=".2f",
            cmap='coolwarm',
            cbar=True,
            square=True,
            cbar_ax=cbar_ax)
ax1.set_title('BERT Heatmap', fontsize = 14)
ax1.tick_params(axis='both', labelsize=12)  # increase tick label size


# Generate a heatmap for LDA
sns.heatmap(spearman_corr_LDA,
            ax=ax2,
            annot=True,
            fmt=".2f",
            cmap='coolwarm',
            cbar=False,  # remove colorbar
            square=True)
ax2.set_title('LDA Heatmap', fontsize = 14)
ax2.tick_params(axis='both', labelsize=12)  # increase tick label size



plt.tight_layout()

fig.suptitle('Spearman\'s Rank Correlation Heatmaps for BERT and LDA', fontsize=16, fontweight='bold')


plt.savefig('/content/drive/MyDrive/Thesis Data Science Python/heatmaps_spearman.jpg', format='jpg', dpi=300, bbox_inches='tight', pad_inches = 0)

# Show the figure
plt.show()

full_base = pd.concat([X_train_base, X_val_base, X_test_base], axis = 0)

# List of dataframes and their names
dfs = [(full_base, 'base'),(full_2l, "LDA"), (full_2b, "BERT")]

# Loop over dataframes and print descriptive statistics
for df, name in dfs:
    print(f"Descriptive statistics for {name} features:\n", df.describe(), "\n")

"""# Baseline models

## Baseline Support Vector Regressor & Random Forest Regressor
"""

def baseline_model(X_test_base, y_test_base, scaler):

    # Inverse transform if the data was previously scaled
    baseline_pred_inverse = scaler.inverse_transform(X_test_base.values.reshape(-1, 1))
    y_test_base_inverse = scaler.inverse_transform(y_test_base.values.reshape(-1, 1))

    # Calculate metrics with the inverse transformed values
    mae = mean_absolute_error(y_test_base_inverse, baseline_pred_inverse)
    rmse = sqrt(mean_squared_error(y_test_base_inverse, baseline_pred_inverse))
    mape = mean_absolute_percentage_error(y_test_base_inverse, baseline_pred_inverse)

    return mae, rmse, mape

# Usage
mae, rmse, mape = baseline_model(X_test_base, y_test_base, scaler)
print(f"Baseline MAE: {mae}, RMSE: {rmse}, MAPE: {mape}")

def test_rf(X_train, X_val, y_train, y_val, feature_set, tscv):
    # create empty results list to store results of each split
    results_list = []

    # The TimeSeriesSplit is applied only on the validation set
    for val_train_index, val_test_index in tscv.split(X_val):
        X_val_train, X_val_test = X_val.iloc[val_train_index], X_val.iloc[val_test_index]
        y_val_train, y_val_test = y_val.iloc[val_train_index], y_val.iloc[val_test_index]

        # The training set remains the same for all splits
        X_train_final = pd.concat([X_train, X_val_train])
        y_train_final = pd.concat([y_train, y_val_train])

        model = RandomForestRegressor(random_state=42)
        model.fit(X_train_final, y_train_final.values.ravel())
        y_pred = model.predict(X_val_test)

        y_val_test_inverse = scaler.inverse_transform(y_val_test.values.reshape(-1, 1))
        y_pred_inverse = scaler.inverse_transform(y_pred.reshape(-1, 1))

        mae = mean_absolute_error(y_val_test_inverse, y_pred_inverse)
        rmse = sqrt(mean_squared_error(y_val_test_inverse, y_pred_inverse))
        mape = mean_absolute_percentage_error(y_val_test_inverse, y_pred_inverse)

        result = {
            'Algorithm': 'RandomForest',
            'Feature set': feature_set,
            'MAE': mae,
            'RMSE': rmse,
            'MAPE': mape,  # include MAPE in the result, if necessary
        }
        results_list.append(result)

    results_df = pd.DataFrame(results_list)
    average_result = {
        'Algorithm': 'RandomForest',
        'Feature set': feature_set,
        'Average MAE': results_df['MAE'].mean(),
        'Average RMSE': results_df['RMSE'].mean(),
        'Average MAPE': results_df['MAPE'].mean(),  # include average MAPE in the result
    }

    return pd.DataFrame([average_result]), y_pred_inverse


# Create TimeSeriesSplit object
tscv = TimeSeriesSplit(n_splits=5)

results_rf = []

results_rf_subset2b, y_pred_rf_subset2b = test_rf(X_train_2b, X_val_2b, y_train_2b, y_val_2b, 'BERTopic_subset_2', tscv)
results_rf_subset2l, y_pred_rf_subset2l = test_rf(X_train_2l, X_val_2l, y_train_2l, y_val_2l, 'LDA_subset_2', tscv)
results_rf_base, y_pred_rf_base = test_rf(X_train_base, X_val_base, y_train_base, y_val_base, 'Base', tscv)

results_rf.append(results_rf_subset2l)
results_rf.append(results_rf_subset2b)
results_rf.append(results_rf_base)

results_rf_testing = pd.concat(results_rf, ignore_index=True)
results_rf_testing

"""## Baseline LSTM model"""

# function to create sequences for lookback period
def create_sequences(X, y, time_steps=10):
    Xs, ys = [], []
    for i in range(len(X) - time_steps):
        Xs.append(X.iloc[i:(i + time_steps)].values)
        ys.append(y.iloc[i + time_steps])
    return np.array(Xs), np.array(ys)

def LSTM_model(X_train, X_val, y_train, y_val, feature_set, tscv, time_steps):
    results_list = []

    # Create sequences
    X_train_seq, y_train_seq = create_sequences(X_train, y_train, time_steps)
    X_val_seq, y_val_seq = create_sequences(X_val, y_val, time_steps)

    # Apply the TimeSeriesSplit only on the validation set
    for val_train_index, val_test_index in tscv.split(X_val_seq):
        X_val_train, X_val_test = X_val_seq[val_train_index], X_val_seq[val_test_index]
        y_val_train, y_val_test = y_val_seq[val_train_index], y_val_seq[val_test_index]

        # The training set remains the same for all splits
        X_train_final = np.concatenate([X_train_seq, X_val_train])
        y_train_final = np.concatenate([y_train_seq, y_val_train])

        # Define the LSTM model
        model = Sequential()
        model.add(LSTM(4, input_shape=(X_train_final.shape[1], X_train_final.shape[2])))
        model.add(Dropout(0.05))
        model.add(Dense(1))
        model.compile(loss='mae', optimizer='adam')

        # Fit network
        history = model.fit(X_train_final, y_train_final, epochs=100, batch_size=72, validation_data=(X_val_test, y_val_test), verbose=0, shuffle=False)

        # Prediction
        y_pred = model.predict(X_val_test)

        # Inverse transform the y_val_test and y_pred
        y_val_test_inverse = scaler.inverse_transform(y_val_test.reshape(-1, 1))
        y_pred_inverse = scaler.inverse_transform(y_pred)

        # Calculate metrics
        rmse = np.sqrt(mean_squared_error(y_val_test_inverse, y_pred_inverse))
        mae = mean_absolute_error(y_val_test_inverse, y_pred_inverse)
        mape = mean_absolute_percentage_error(y_val_test_inverse, y_pred_inverse)

        result = {
            'Algorithm': 'RandomForest',
            'Feature set': feature_set,
            'MAE': mae,
            'RMSE': rmse,
            'MAPE': mape,
        }
        results_list.append(result)

    results_df = pd.DataFrame(results_list)
    average_result = {
        'Algorithm': 'LSTM',
        'Feature set': feature_set,
        'Average MAE': results_df['MAE'].mean(),
        'Average RMSE': results_df['RMSE'].mean(),
        'Average MAPE': results_df['MAPE'].mean()
    }

    return pd.DataFrame([average_result])


# Create TimeSeriesSplit object
tscv = TimeSeriesSplit(n_splits=5)

results = []
time_steps = 10

results.append(LSTM_model(X_train_base, X_val_base, y_train_base, y_val_base, 'Base', tscv, time_steps))
results.append(LSTM_model(X_train_2b, X_val_2b, y_train_2b, y_val_2b, 'BERTopic_subset2', tscv, time_steps))
results.append(LSTM_model(X_train_2l, X_val_2l, y_train_2l, y_val_2l, 'LDA_subset2', tscv, time_steps))

results_LSTM = pd.concat(results, ignore_index=True)
results_LSTM

"""# Hyperparameter tuning

### RF & SVR Hyperparameter tuning
"""

# Hyperparameters to tune
param_dist_rf = {
    'n_estimators': [5, 15, 25, 50, 100, 200],
    'max_depth': [None, 5, 15, 25, 35],
    'min_samples_split': [5, 10, 15, 20, 25, 30]
}

tscv = TimeSeriesSplit(n_splits=5)

# Function to tune hyperparameters
def tune_hyperparameters(X_train, X_val, y_train, y_val, feature_set):

    rf = RandomForestRegressor()
    rf_random = RandomizedSearchCV(estimator=rf, param_distributions=param_dist_rf,
                                   n_iter=100, verbose=2, random_state=42,
                                   n_jobs=-1, cv = tscv)

    results_rf = []
    results_svr = []

    for algorithm in [rf_random]:
        algorithm_name = algorithm.estimator.__class__.__name__  # Access the underlying model
        algorithm.fit(X_train, y_train)
        y_pred = algorithm.predict(X_val)

        y_val_inverse = scaler.inverse_transform(y_val.values.reshape(-1, 1))
        y_pred_inverse = scaler.inverse_transform(y_pred.reshape(-1, 1))

        mae = mean_absolute_error(y_val_inverse, y_pred_inverse)
        rmse = sqrt(mean_squared_error(y_val_inverse, y_pred_inverse))
        mape = mean_absolute_percentage_error(y_val_inverse, y_pred_inverse)  # compute MAPE


        result = {
            'Algorithm': algorithm_name,
            'Feature Set': feature_set,
            'Best Hyperparameters': algorithm.best_params_,
            'MAE': mae,
            'RMSE': rmse,
            'MAPE': mape
        }

        if algorithm_name == 'RandomForestRegressor':
            results_rf.append(result)


    return results_rf


results_rf_tuning = []
results_svr_tuning = []


rf_tuning = tune_hyperparameters(X_train_2b, X_val_2b, y_train_2b, y_val_2b, 'BERTopic_subset_2')
results_rf_tuning += rf_tuning


rf_tuning = tune_hyperparameters(X_train_2l, X_val_2l, y_train_2l, y_val_2l, 'LDA_subset_2')
results_rf_tuning += rf_tuning

rf_tuning = tune_hyperparameters(X_train_base, X_val_base, y_train_base, y_val_base, 'Base')
results_rf_tuning += rf_tuning


results_rf_tuning = pd.DataFrame(results_rf_tuning)
results_rf_tuning

"""## LSTM parameter tuning"""

# function to build LSTM model
def build_model(hp):
    model = keras.Sequential()
    model.add(layers.LSTM(units=hp.Choice('units', [4, 8, 16, 32, 64, 128, 256])))
    model.add(layers.Dropout(hp.Choice('dropout', [0.05, 0.15, 0.25, 0.5, 0.75])))
    model.add(layers.Dense(1, activation=hp.Choice('dense_activation', ['linear'])))
    model.compile(
        optimizer=keras.optimizers.Adam(
            hp.Choice('learning_rate', [0.01, 0.001, 0.0001])),
        loss='mae')
    return model


time_steps = 10

    # Define function to evaluate model
def evaluate_model(model, X, y_val):
    y_pred = model.predict(X)
    y_val_inverse = scaler.inverse_transform(y_val.reshape(-1, 1))
    y_pred_inverse = scaler.inverse_transform(y_pred.reshape(-1, 1))
    mae = mean_absolute_error(y_val_inverse, y_pred_inverse)
    rmse = sqrt(mean_squared_error(y_val_inverse, y_pred_inverse))
    mape = mean_absolute_percentage_error(y_val_inverse, y_pred_inverse)  # compute MAPE
    return mae, rmse, mape

# Helper function to extract hyperparameters from a HyperParameters object
def extract_hyperparameters(hp):
    return {h.name: h.default for h in hp.space}

# define early stopping
early_stopping = tf.keras.callbacks.EarlyStopping(
    monitor='val_loss',
    patience=10,
    restore_best_weights=True
)

def LSTM_tuning(X_train, X_test, y_train, y_test, feature_set, time_steps):

    X_train_LSTM, y_train_LSTM = create_sequences(X_train, y_train, time_steps)
    X_test_LSTM, y_test_LSTM = create_sequences(X_test, y_test, time_steps)


# Get the Keras tuner in place
    tuner = Hyperband(
        build_model,
        objective='val_loss',
        max_epochs=50,
        directory=feature_set,
        project_name='oil_price_3')

    # Deploy the tuner to the data
    tuner.search(X_train_LSTM, y_train_LSTM, epochs=50, validation_data=(X_test_LSTM, y_test_LSTM),callbacks=[early_stopping])

    # Get best models
    best_model = tuner.get_best_models(num_models=1)[0]

    #  Evaluate models
    mae, rmse, mape = evaluate_model(best_model, X_test_LSTM, y_test_LSTM)

    # Retrieve best hyperparameters
    best_hyperparameters = tuner.get_best_hyperparameters(num_trials=50)[0]

    # Create df
    result = pd.DataFrame({
        'Algorithm': ['LSTM'],
        'Feature set': [feature_set],
        'MAE': [mae],
        'RMSE': [rmse],
        'MAPE': [mape],
        'Hyperparameters':[str(extract_hyperparameters(best_hyperparameters))]
    })

    return result, best_hyperparameters


results = []


results_subset2b, best_hps_subset2b = LSTM_tuning(X_train_2b, X_val_2b, y_train_2b, y_val_2b, 'BERTopic_subset_2_tun', time_steps)
results_subset2l, best_hps_subset2l = LSTM_tuning(X_train_2l, X_val_2l, y_train_2l, y_val_2l, 'LDA_subset_2_tun', time_steps)
results_base, best_hps_base = LSTM_tuning(X_train_base, X_val_base, y_train_base, y_val_base, 'base_tun', time_steps)

results.append(results_subset2b)
results.append(results_subset2l)
results.append(results_base)

results_LSTM_tuning = pd.concat(results, ignore_index=True)

results_LSTM_tuning

results_LSTM_tuning.to_csv('/content/drive/MyDrive/Thesis Data Science Python/LSTM_hyperparamters.csv')

"""# Testing the tuned models

## RF
"""

def validate_rf_model(X_train, X_val, X_test, y_train, y_val, y_test, best_params, feature_set, dates):
    results_list = []
    all_y_pred = np.array([])  # initialize empty numpy array to store all predictions
    all_y_test = np.array([])

       # Concatenate
    X_train = np.concatenate((X_train, X_val), axis=0)
    y_train = np.concatenate((y_train, y_val), axis=0)

    maes = []
    rmses = []
    mape = []

    # The TimeSeriesSplit is applied only on the validation set
    for test_train_index, test_test_index in tscv.split(X_test):

        X_test_train, X_test_test = X_test.iloc[test_train_index], X_test.iloc[test_test_index]
        y_test_train, y_test_test = y_test.iloc[test_train_index], y_test.iloc[test_test_index]

        # The training set remains the same for all splits

        X_train_final = np.concatenate([X_train, X_test_train], axis = 0)

        y_train_final = np.concatenate([y_train, y_test_train], axis = 0)

        model = RandomForestRegressor(**best_params)


        model.fit(X_train, y_train.ravel())
        y_pred = model.predict(X_test_test)

        y_test_inverse = scaler.inverse_transform(y_test_test.values.reshape(-1, 1))
        y_pred_inverse = scaler.inverse_transform(y_pred.reshape(-1, 1))
        mae = mean_absolute_error(y_test_inverse, y_pred_inverse)
        rmse = sqrt(mean_squared_error(y_test_inverse, y_pred_inverse))
        mape = mean_absolute_percentage_error(y_test_inverse, y_pred_inverse)

        maes.append(mae)
        rmses.append(rmse)

        print(feature_set)
        print(mae)
        print(rmse)

        all_y_pred = np.append(all_y_pred, y_pred_inverse)
        all_y_test = np.append(all_y_test, y_test_inverse)


    # Create a dataframe to store the dates, actuals, predictions and errors
        results = pd.DataFrame({
            'date': dates[test_test_index], # here we only get the dates for the current split
            'actual': y_test_inverse.flatten(),
            'prediction': y_pred_inverse.flatten(),
            'abs_error': np.abs(y_test_inverse.flatten() - y_pred_inverse.flatten()),
            'squared_error': (y_test_inverse.flatten() - y_pred_inverse.flatten()) ** 2,
    })
        results_list.append(results)


    # At the end of your function, concatenate all results DataFrames into one
    final_results = pd.concat(results_list, axis=0)
    final_results['Algorithm'] = 'RF'
    final_results['Feature set'] = feature_set
    final_results['MAE'] = np.mean(maes)
    final_results['RMSE'] = np.mean(rmses)
    final_results['R-squared'] = np.mean(mape)  # include R-squared score in the result

    return np.mean(maes), np.mean(rmses), mape, final_results, all_y_pred, all_y_test


# Extract datetime index from the test data
test_dates = X_test_2b.index


# function to get seasons from datetime
def get_season(date):
    month = date.month
    if 3 <= month <= 5:
        return 'Spring'
    elif 6 <= month <= 8:
        return 'Summer'
    elif 9 <= month <= 11:
        return 'Fall'
    else:
        return 'Winter'


# getting the parameter
best_params_rf_subset2b = results_rf_tuning.loc[results_rf_tuning['Feature Set'] == 'BERTopic_subset_2', 'Best Hyperparameters'].values[0]#
best_params_rf_subset2l = results_rf_tuning.loc[results_rf_tuning['Feature Set'] == 'LDA_subset_2', 'Best Hyperparameters'].values[0]
best_params_rf_base = results_rf_tuning.loc[results_rf_tuning['Feature Set'] == 'Base', 'Best Hyperparameters'].values[0]

# Apply the model validation and get error data per season
mae_rf_subset_2l, rmse_rf_subset_2l, mape_rf_subset_2l, results_rf_subset_2l, y_pred_2l_rf, y_test_2l_rf = validate_rf_model(X_train_2l, X_val_2l, X_test_2l, y_train_2l, y_val_2l, y_test_2l, best_params_rf_subset2l, 'LDA', test_dates)
mae_rf_subset_2b, rmse_rf_subset_2b, mape_rf_subset_2b, results_rf_subset_2b, y_pred_2b_rf, y_test_2b_rf= validate_rf_model(X_train_2b, X_val_2b, X_test_2b, y_train_2b, y_val_2b, y_test_2b, best_params_rf_subset2b,'BERT', test_dates)
mae_rf_base, rmse_rf_base, mape_rf_base, results_rf_base, y_pred_base_rf, y_test_base_rf = validate_rf_model(X_train_base, X_val_base, X_test_base, y_train_base, y_val_base, y_test_base, best_params_rf_base, 'BASE', test_dates)

# Add 'season' column to the results
results_rf_subset_2l['season'] = results_rf_subset_2l['date'].apply(get_season)
results_rf_subset_2b['season'] = results_rf_subset_2b['date'].apply(get_season)
results_rf_base['season'] = results_rf_base['date'].apply(get_season)

# Calculate mean absolute and squared error per season
error_data_rf_subset_2l = results_rf_subset_2l.groupby('season')[['abs_error', 'squared_error']].mean().reset_index()
error_data_rf_subset_2b = results_rf_subset_2b.groupby('season')[['abs_error', 'squared_error']].mean().reset_index()
error_data_rf_base = results_rf_base.groupby('season')[['abs_error', 'squared_error']].mean().reset_index()

# Add 'season' column to the results
results_rf_subset_2l['season'] = results_rf_subset_2l['date'].apply(get_season)
results_rf_subset_2b['season'] = results_rf_subset_2b['date'].apply(get_season)
results_rf_base['season'] = results_rf_base['date'].apply(get_season)

# Calculate mean absolute and squared error per season
error_data_rf_subset_2l = results_rf_subset_2l.groupby('season')[['abs_error', 'squared_error']].mean().reset_index()
error_data_rf_subset_2b = results_rf_subset_2b.groupby('season')[['abs_error', 'squared_error']].mean().reset_index()
error_data_rf_base = results_rf_base.groupby('season')[['abs_error', 'squared_error']].mean().reset_index()

print(error_data_rf_subset_2l)
print(error_data_rf_subset_2b)
print(error_data_rf_base)

# For MAE
avg_mae_rf_subset_2l = error_data_rf_subset_2l['abs_error'].mean()
avg_mae_rf_subset_2b = error_data_rf_subset_2b['abs_error'].mean()
avg_mae_rf_base = error_data_rf_base['abs_error'].mean()

print("Average MAE for RF subset 2l: ", avg_mae_rf_subset_2l)
print("Average MAE for RF subset 2b: ", avg_mae_rf_subset_2b)
print("Average MAE for RF base: ", avg_mae_rf_base)

# For RMSE
avg_rmse_rf_subset_2l = np.sqrt(error_data_rf_subset_2l['squared_error'].mean())
avg_rmse_rf_subset_2b = np.sqrt(error_data_rf_subset_2b['squared_error'].mean())
avg_rmse_rf_base = np.sqrt(error_data_rf_base['squared_error'].mean())

print("\nAverage RMSE for RF subset 2l: ", avg_rmse_rf_subset_2l)
print("Average RMSE for RF subset 2b: ", avg_rmse_rf_subset_2b)
print("Average RMSE for RF base: ", avg_rmse_rf_base)

"""## LSTM"""

tscv = TimeSeriesSplit(n_splits=5)
results = []
time_steps = 10

def create_sequences(X, y, time_steps):
    Xs, ys = [], []
    for i in range(len(X) - time_steps):
        Xs.append(X.iloc[i:(i + time_steps)].values)
        ys.append(y.iloc[i + time_steps - 1])
    return np.array(Xs), np.array(ys)


def validate_lstm_model(best_hps, X_train, X_val, X_test, y_train, y_val, y_test, feature_set, tscv, dates):
    results_list = []
    all_y_pred = np.array([])
    all_y_test = np.array([])

    # Create sequences from the training data
    X_train_seq, y_train_seq = create_sequences(X_train, y_train, time_steps)
    X_val_seq, y_val_seq = create_sequences(X_val, y_val, time_steps)
    X_test_seq, y_test_seq = create_sequences(X_test, y_test, time_steps)

    # Concatenate
    X_train = np.concatenate((X_train_seq, X_val_seq), axis=0)
    y_train = np.concatenate((y_train_seq, y_val_seq), axis=0)

    maes = []
    rmses = []
    mapes = []

    for val_train_index, val_test_index in tscv.split(X_test_seq):
        X_val_train, X_val_test = X_test_seq[val_train_index], X_test_seq[val_test_index]
        y_val_train, y_val_test = y_test_seq[val_train_index], y_test_seq[val_test_index]

        # The training set remains the same for all splits
        X_train_final = np.concatenate([X_train, X_val_train])
        y_train_final = np.concatenate([y_train, y_val_train])

        # Define the LSTM model
        model = keras.Sequential()
        model.add(layers.LSTM(units=best_hps.get('units')))
        model.add(layers.Dropout(best_hps.get('dropout')))
        model.add(layers.Dense(1, activation=best_hps.get('dense_activation')))
        model.compile(
            optimizer=keras.optimizers.Adam(
                best_hps.get('learning_rate')),
            loss='mae')

        # Fit network
        history = model.fit(X_train_final, y_train_final, epochs=100, batch_size=72, validation_data=(X_val_test, y_val_test), verbose=0, shuffle=False)

        # Prediction
        y_pred = model.predict(X_val_test)

        # Inverse transform the y_val_test and y_pred
        y_test_inverse = scaler.inverse_transform(y_val_test.reshape(-1, 1))
        y_pred_inverse = scaler.inverse_transform(y_pred)

        # Calculate metrics
        rmse = np.sqrt(mean_squared_error(y_test_inverse, y_pred_inverse))
        mae = mean_absolute_error(y_test_inverse, y_pred_inverse)
        mape = mean_absolute_percentage_error(y_test_inverse, y_pred_inverse)  # compute MAPE


        maes.append(mae)
        rmses.append(rmse)
        mapes.append(mape)
        print(feature_set)
        print(mae)
        print(rmse)


        all_y_pred = np.append(all_y_pred, y_pred_inverse)  # append the predictions to the array
        all_y_test = np.append(all_y_test, y_test_inverse)


       # create df
        results = pd.DataFrame({
            'date': dates[val_test_index],
            'actual': y_test_inverse.flatten(),
            'prediction': y_pred_inverse.flatten(),
            'abs_error': np.abs(y_test_inverse.flatten() - y_pred_inverse.flatten()),
            'squared_error': (y_test_inverse.flatten() - y_pred_inverse.flatten()) ** 2,
    })
        results_list.append(results)



    final_results = pd.concat(results_list, axis=0)
    final_results['Algorithm'] = 'RF'
    final_results['Feature set'] = feature_set
    final_results['MAE'] = np.mean(maes)
    final_results['RMSE'] = np.mean(rmses)
    final_results['R-squared'] = np.mean(mape)

    return np.mean(maes), np.mean(rmses), mape, final_results, all_y_pred, all_y_test



# Extract datetime index from the test data
test_dates = X_test_2b.index



# Apply the model validation and get error data per season
mae_lstm_subset_2l, rmse_lstm_subset_2l, mape_lstm_subset_2l, results_lstm_subset_2l, y_pred_2l, y_test_2l = validate_lstm_model(best_hps_subset2l, X_train_2l, X_val_2l, X_test_2l, y_train_2l, y_val_2l, y_test_2l, 'LDA', tscv, test_dates)
mae_lstm_subset_2b, rmse_lstm_subset_2b, mape_lstm_subset_2b, results_lstm_subset_2b, y_pred_2b, y_test_2b= validate_lstm_model(best_hps_subset2b, X_train_2b, X_val_2b, X_test_2b, y_train_2b, y_val_2b, y_test_2b,'BERT',tscv,  test_dates)
mae_lstm_base, rmse_lstm_base, mape_lstm_base, results_lstm_base, y_pred_base, y_test_base = validate_lstm_model(best_hps_base, X_train_base, X_val_base, X_test_base, y_train_base, y_val_base, y_test_base, 'BASE',tscv,  test_dates)

# add 'seasons'
results_lstm_subset_2l['season'] = results_lstm_subset_2l['date'].apply(get_season)
results_lstm_subset_2b['season'] = results_lstm_subset_2b['date'].apply(get_season)
results_lstm_base['season'] = results_lstm_base['date'].apply(get_season)

# Calculate mean absolute and squared error per day of the week
error_data_lstm_subset_2l = results_lstm_subset_2l.groupby('season')[['abs_error', 'squared_error']].mean().reset_index()
error_data_lstm_subset_2b = results_lstm_subset_2b.groupby('season')[['abs_error', 'squared_error']].mean().reset_index()
error_data_lstm_base = results_lstm_base.groupby('season')[['abs_error', 'squared_error']].mean().reset_index()

avg_mae_lstm_subset_2l = error_data_lstm_subset_2l['abs_error'].mean()
avg_mae_lstm_subset_2b = error_data_lstm_subset_2b['abs_error'].mean()
avg_mae_lstm_base = error_data_lstm_base['abs_error'].mean()

print("Average MAE for LSTM subset 2l: ", avg_mae_lstm_subset_2l)
print("Average MAE for LSTM subset 2b: ", avg_mae_lstm_subset_2b)
print("Average MAE for LSTM base: ", avg_mae_lstm_base)

avg_rmse_lstm_subset_2l = error_data_lstm_subset_2l['squared_error'].mean()
avg_rmse_lstm_subset_2b = error_data_lstm_subset_2b['squared_error'].mean()
avg_rmse_lstm_base = error_data_lstm_base['squared_error'].mean()

print("Average RMSE for LSTM subset 2l: ", avg_rmse_lstm_subset_2l)
print("Average RMSE for LSTM subset 2b: ", avg_rmse_lstm_subset_2b)
print("Average RMSE for LSTM base: ", avg_rmse_lstm_base)

# Add 'season' column to the results
results_lstm_subset_2l['season'] = results_lstm_subset_2l['date'].apply(get_season)
results_lstm_subset_2b['season'] = results_lstm_subset_2b['date'].apply(get_season)
results_lstm_base['season'] = results_lstm_base['date'].apply(get_season)

# Calculate mean absolute and squared error per season
error_data_lstm_subset_2l = results_lstm_subset_2l.groupby('season')[['abs_error', 'squared_error']].mean().reset_index()
error_data_lstm_subset_2b = results_lstm_subset_2b.groupby('season')[['abs_error', 'squared_error']].mean().reset_index()
error_data_lstm_base = results_lstm_base.groupby('season')[['abs_error', 'squared_error']].mean().reset_index()

print(error_data_lstm_subset_2l)
print(error_data_lstm_subset_2b)
print(error_data_lstm_base)

"""# Plotting the results

### Residual Plots
"""

# Function to plot residuals
def plot_residuals(y_test, y_preds, model_names):
    fig, axs = plt.subplots(1, 3, figsize=(15, 5))

    # Set a common title for the entire figure
    fig.suptitle('Error plots for Random Forest model', fontsize=20, fontweight='bold')

    # Instantiate an engineering formatter
    eng_format = EngFormatter()

    for ax, y_pred, model_name in zip(axs, y_preds, model_names):
        residuals = y_test - y_pred
        scatter = ax.scatter(y_pred, residuals, edgecolors='k', alpha=0.6)
        ax.axhline(y=0, color='r', linestyle='-', linewidth=1.2)
        ax.set_title(model_name, fontsize=18)
        ax.set_xlabel('Predicted values oil price',fontsize=16)
        ax.set_ylabel('Errors', fontsize=16)

        # Improve number readability
        ax.xaxis.set_major_formatter(eng_format)
        ax.yaxis.set_major_formatter(eng_format)

        # Add grid
        ax.grid(True, linestyle='--', alpha=0.6)

    # Improve layout
    plt.tight_layout()
    plt.subplots_adjust(top=0.85)  # Adjust top to accommodate the common title

    # Save the figure
    fig.savefig('/content/drive/MyDrive/Thesis Data Science Python/residual_plots_rf.pdf', dpi=300, bbox_inches='tight')

    plt.show()

# Inverse transformation of y_val

# Convert DataFrame to Series
y_test_2b_series = y_test_2b.iloc[:, 0]

# Call the function
plot_residuals(y_test_2b_series, [y_pred_base_rf, y_pred_2l_rf, y_pred_2b_rf], ['Baseline', 'LDA', 'BERTopic'])

plt.style.use('ggplot')  # Use a different style

def plot_residuals(y_test, y_preds, model_names):
    fig, axs = plt.subplots(1, 3, figsize=(20, 8))  # Increase figure size

    fig.suptitle('Residual Error Plots for Different Models', fontsize=20, fontweight='bold')

    eng_format = EngFormatter()

    for ax, y_pred, model_name in zip(axs, y_preds, model_names):
        residuals = y_test - y_pred
        scatter = ax.scatter(y_pred, residuals, c=residuals, cmap='viridis', edgecolors='w', alpha=0.6)  # Color points based on density
        ax.axhline(y=0, color='red', linestyle='-', linewidth=1.5)
        ax.set_title(model_name, fontsize=18, fontweight='bold')
        ax.set_xlabel('Predicted Oil Price (in dollars)', fontsize=16)
        ax.set_ylabel('Prediction Errors (in dollars)', fontsize=16)
        ax.xaxis.set_major_formatter(eng_format)
        ax.yaxis.set_major_formatter(eng_format)
        ax.grid(True, linestyle='--', alpha=0.6)

    plt.tight_layout()
    plt.subplots_adjust(top=0.9)

    fig.savefig('/content/drive/MyDrive/Thesis Data Science Python/residual_plots_lstm.pdf', dpi=300, bbox_inches='tight')

    plt.show()


# Create residual plots
plot_residuals(y_test_2l,
               [ y_pred_base, y_pred_2l, y_pred_2b],
               ['Baseline', 'LDA', 'BERTopic'])

# Function to plot residuals
def plot_residuals(y_test, y_preds, model_names):
    fig, axs = plt.subplots(1, 3, figsize=(15, 5))

    # Set a common title for the entire figure
    fig.suptitle('Error plots for Different Models', fontsize=18)

    # Instantiate an engineering formatter
    eng_format = EngFormatter()

    for ax, y_pred, model_name in zip(axs, y_preds, model_names):
        residuals = y_test - y_pred
        scatter = ax.scatter(y_pred, residuals, edgecolors='k', alpha=0.6)
        ax.axhline(y=0, color='r', linestyle='-', linewidth=1.2)
        ax.set_title(model_name, fontsize=18)
        ax.set_xlabel('Predicted values oil price', fontsize=16)
        ax.set_ylabel('Errors', fontsize=16)

        # Improve number readability
        ax.xaxis.set_major_formatter(eng_format)
        ax.yaxis.set_major_formatter(eng_format)

        # Add grid
        ax.grid(True, linestyle='--', alpha=0.6)

    # Improve layout
    plt.tight_layout()
    plt.subplots_adjust(top=0.85)  # Adjust top to accommodate the common title

    # Save the figure
    fig.savefig('/content/drive/MyDrive/Thesis Data Science Python/residual_plots_lstm.pdf', dpi=300, bbox_inches='tight')

    plt.show()

# Inverse transformation of y_val

# Create residual plots
plot_residuals(y_test_LSTM_base,
               [ y_pred_LSTM_base, y_pred_LSTM_subset2l, y_pred_LSTM_subset2b],
               ['Baseline', 'LDA', 'BERTopic'])

# Importing additional library
from matplotlib.ticker import EngFormatter

# Create a figure with two subplots vertically arranged
fig, axs = plt.subplots(2, 1, figsize=(16, 14))

# Set a common title for the entire figure
fig.suptitle('Actual Versus Predicted Values for Random Forest and LSTM Models', fontsize=24, fontweight='bold')

# Calculate the 50% mark for data
data_len = len(y_test_2l)
fifty_percent_mark = int(data_len * 0.5)

# Create date range for 50% of the data
date_range = pd.date_range(start='01-01-2017', periods=fifty_percent_mark)

# Instantiate an engineering formatter
eng_format = EngFormatter()

# Plotting in the first subplot for Random Forest
axs[0].plot(date_range, y_test_2l_rf[:fifty_percent_mark], label='Actual values', linewidth=2)
axs[0].plot(date_range, y_pred_base_rf[:fifty_percent_mark], label='Baseline predictions', linewidth=2)
axs[0].plot(date_range, y_pred_2l_rf[:fifty_percent_mark], label='LDA predictions', linewidth=2)
axs[0].plot(date_range, y_pred_2b_rf[:fifty_percent_mark], label='BERTopic predictions', linewidth=2)

axs[0].set_title('Random Forest Model', fontsize=20)
axs[0].set_xlabel('Date', fontsize=16)
axs[0].set_ylabel('Oil price', fontsize=16)
axs[0].legend(loc='upper left', fontsize=14)

# Improve number readability
axs[0].yaxis.set_major_formatter(eng_format)

# Add grid
axs[0].grid(True, linestyle='--', alpha=0.6)

# Plotting in the second subplot for LSTM
axs[1].plot(date_range, y_test_2l_rf[:fifty_percent_mark], label='Actual values', linewidth=1.5)
axs[1].plot(date_range, y_pred_base[:fifty_percent_mark], label='Baseline predictions', linewidth=1.5)
axs[1].plot(date_range, y_pred_2l[:fifty_percent_mark], label='LDA predictions', linewidth=1.5)
axs[1].plot(date_range, y_pred_2b[:fifty_percent_mark], label='BERTopic predictions', linewidth=1.5)

axs[1].set_title('LSTM Model', fontsize=20)
axs[1].set_xlabel('Date', fontsize=16)
axs[1].set_ylabel('Oil price', fontsize=16)
axs[1].legend(loc='upper left', fontsize=14)

# Improve number readability
axs[1].yaxis.set_major_formatter(eng_format)

# Add grid
axs[1].grid(True, linestyle='--', alpha=0.6)

# Improve layout
fig.tight_layout(pad=3.0)
plt.subplots_adjust(top=0.9)  # Adjust top to accommodate the common title

# Save the figure
fig.savefig('/content/drive/MyDrive/Thesis Data Science Python/predvsact_02.pdf', format='pdf', dpi=300)

plt.show()

"""# Error Analysis

# T-test & Cohen's D
"""

def check_normality(df_basic, df_extended, mae_column='MAE'):
    # merge the dataframes on the appropriate index
    df = pd.merge(df_basic[[mae_column]], df_extended[[mae_column]],
                  left_index=True, right_index=True, suffixes=('_basic', '_extended'))

    # calculate the differences in MAE
    df['diff_mae'] = df[mae_column + '_basic'] - df[mae_column + '_extended']

    # conduct Shapiro-Wilk test
    stat, p = stats.shapiro(df['diff_mae'])
    print('Statistics=%.3f, p=%.3f' % (stat, p))

    # interpret the result
    alpha = 0.05
    if p > alpha:
        print('Sample looks Gaussian (fail to reject H0)')
    else:
        print('Sample does not look Gaussian (reject H0)')

    # create histogram
    plt.figure(figsize=(10,5))
    plt.subplot(1, 2, 1)
    sns.histplot(df['diff_mae'], kde=True)
    plt.title('Histogram')

    plt.tight_layout()
    plt.show()

check_normality(results_lstm_base, results_lstm_subset_2b, mae_column='squared_error')

check_normality(results_lstm_base, results_lstm_subset_2l, mae_column='squared_error')

check_normality(results_rf_base, results_rf_subset_2b, mae_column='squared_error')

check_normality(results_rf_base, results_rf_subset_2l, mae_column='squared_error')

"""## RF model"""

def calculate_cohen_d(df1, df2, error_col):
    mean1, mean2 = df1[error_col].mean(), df2[error_col].mean()
    n1, n2 = len(df1), len(df2)
    std1, std2 = df1[error_col].std(), df2[error_col].std()
    pooled_std = np.sqrt(((n1 - 1) * std1**2 + (n2 - 1) * std2**2) / (n1 + n2 - 2))
    print(pooled_std)
    d = (mean1 - mean2) / pooled_std
    return d

# Calculate Cohen's d for MAE between Baseline and LDA
print("Cohen's d for MAE between Baseline and LDA: ", calculate_cohen_d(results_rf_base, results_rf_subset_2l, 'abs_error'))

# Calculate Cohen's d for MAE between Baseline and BERTopic
print("Cohen's d for MAE between Baseline and BERTopic: ", calculate_cohen_d(results_rf_base, results_rf_subset_2b, 'abs_error'))

# Calculate Cohen's d for RMSE between Baseline and LDA
print("Cohen's d for RMSE between Baseline and LDA: ", calculate_cohen_d(results_rf_base, results_rf_subset_2l, 'squared_error'))

# Calculate Cohen's d for RMSE between Baseline and BERTopic
print("Cohen's d for RMSE between Baseline and BERTopic: ", calculate_cohen_d(results_rf_base, results_rf_subset_2b, 'squared_error'))

# Perform paired t-tests for MAE
print("Paired t-test for MAE between Baseline and LDA: ", ttest_rel(results_rf_base['abs_error'], results_rf_subset_2l['abs_error']))
print("Paired t-test for MAE between Baseline and BERTopic: ", ttest_rel(results_rf_base['abs_error'], results_rf_subset_2b['abs_error']))

# Perform paired t-tests for RMSE
print("Paired t-test for RMSE between Baseline and LDA: ", ttest_rel(results_rf_base['squared_error'], results_rf_subset_2l['squared_error']))
print("Paired t-test for RMSE between Baseline and BERTopic: ", ttest_rel(results_rf_base['squared_error'], results_rf_subset_2b['squared_error']))

"""## LSTM"""

# Calculate Cohen's d for MAE between Baseline and LDA
print("Cohen's d for MAE between Baseline and LDA: ", calculate_cohen_d(results_lstm_base, results_lstm_subset_2l, 'abs_error'))

# Calculate Cohen's d for MAE between Baseline and BERTopic
print("Cohen's d for MAE between Baseline and BERTopic: ", calculate_cohen_d(results_lstm_base, results_lstm_subset_2b, 'abs_error'))

# Calculate Cohen's d for RMSE between Baseline and LDA
print("Cohen's d for RMSE between Baseline and LDA: ", calculate_cohen_d(results_lstm_base, results_lstm_subset_2l, 'squared_error'))

# Calculate Cohen's d for RMSE between Baseline and BERTopic
print("Cohen's d for RMSE between Baseline and BERTopic: ", calculate_cohen_d(results_lstm_base, results_lstm_subset_2b, 'squared_error'))

# Perform paired t-tests for MAE
print("Paired t-test for MAE between Baseline and LDA: ", ttest_rel(results_lstm_base['abs_error'], results_lstm_subset_2l['abs_error']))
print("Paired t-test for MAE between Baseline and BERTopic: ", ttest_rel(results_lstm_base['abs_error'], results_lstm_subset_2b['abs_error']))

# Perform paired t-tests for RMSE
print("Paired t-test for RMSE between Baseline and LDA: ", ttest_rel(results_lstm_base['squared_error'], results_lstm_subset_2l['squared_error']))
print("Paired t-test for RMSE between Baseline and BERTopic: ", ttest_rel(results_lstm_base['squared_error'], results_lstm_subset_2b['squared_error']))