# -*- coding: utf-8 -*-
"""BERTopic

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XVf8GnBQXaDYwK6v_HSM9JkwvLUyl0RJ

# Install packages
"""

!pip install numpy pandas umap-learn hdbscan bertopic gensim

!pip install sentence-transformers

"""# Load libraries"""

import json 
import pandas as pd
import umap
import numpy as np
import matplotlib.pyplot as plt


import re
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from transformers import DistilBertTokenizer, TFDistilBertModel

from bertopic import BERTopic
import hdbscan
import umap
from umap import UMAP
from hdbscan import HDBSCAN
from tqdm.auto import tqdm

from gensim.corpora import Dictionary
from gensim.models import CoherenceModel
from gensim.utils import simple_preprocess
from gensim.corpora import Dictionary
from gensim.models.coherencemodel import CoherenceModel

import numpy as np
from bertopic import BERTopic
from umap import UMAP
from hdbscan import HDBSCAN
from sklearn.datasets import fetch_20newsgroups
from sklearn.metrics import silhouette_score
from sklearn.feature_extraction.text import CountVectorizer


from bertopic import BERTopic
from bertopic.representation import MaximalMarginalRelevance


from sentence_transformers import SentenceTransformer

from itertools import product



nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

import numpy as np
from umap import UMAP
from bertopic import BERTopic
from sklearn.decomposition import PCA
from sklearn.feature_extraction.text import CountVectorizer
from bertopic.representation import KeyBERTInspired

from google.colab import drive
drive.mount('/content/drive')

"""# Load data"""

headlines_df = pd.read_csv('/content/drive/MyDrive/Thesis Data Science Python/headlines_keywords_paper.csv')

"""# Clean data"""

# Print amount of missing headlines & remove the ones with missing values
print(headlines_df['headline_clean'].isnull().sum())
headlines_df = headlines_df.dropna(subset=['headline_clean'])
headlines_df = headlines_df.dropna(subset=['headline_lemma'])

# Turn all 'headline_clean' into strings
headlines_df['headline_clean'] = headlines_df['headline_clean'].astype(str)
# same here
headlines_df['headline_lemma'] = headlines_df['headline_lemma'].astype(str)

#'date' should be in datetime format
headlines_df['date'] = pd.to_datetime(headlines_df['date'])

# 'date' as index
headlines_df.set_index('date', inplace=True)

# use indexing based on date
start_date = '2009-01-01'
end_date = '2017-01-01'
train_headlines_df = headlines_df.loc[start_date:end_date]

train_headlines_df.dropna()
train_headlines_df.count()

"""## Train the model on the train dataset"""

# Load dataset as list
data = train_headlines_df['headline_clean'].tolist()

# Prepare embeddings (so I don't have to do it over again for each iteration)
docs = train_headlines_df['headline_clean']
sentence_model = SentenceTransformer("all-MiniLM-L6-v2") 
embeddings = sentence_model.encode(docs, show_progress_bar=True)

# Preprocess data for coherence calculation using simple_proprocess function and doc2bow from gensim
tokenized_data = [simple_preprocess(doc) for doc in data]
dictionary = Dictionary(tokenized_data)
corpus = [dictionary.doc2bow(doc) for doc in tokenized_data]

# define vectorizer model
vectorizer_model = CountVectorizer(stop_words="english")

# Representation model to limit number of duplicate words in topics 
representation_model = MaximalMarginalRelevance(diversity=0.3)

# for pca embeddings
def rescale(x, inplace=False):
  if not inplace:
    x = np.array(x, copy=True)
  x /= np.std(x[:, 0]) * 10000
  return x


# Initialize and rescale PCA embeddings
#pca_embeddings = rescale(PCA(n_components=5).fit_transform(embeddings))  # n_components small to decrease computation time

# Parameter ranges
n_neighbors_values = [10, 20, 30]
min_cluster_sizes = [30, 30, 40]
min_topic_sizes = [40, 50, 70]

# Store results with models
results = []

# Iterate through parameter combinations
for n_neighbors, min_cluster_size, min_topic_size in product(n_neighbors_values, min_cluster_sizes, min_topic_sizes):
    # Initialize UMAP model with the current parameters
    umap_model = UMAP(n_neighbors=n_neighbors, n_components=5, metric="cosine", low_memory= True)

    # Initialize HDBSCAN model with the current parameters
    hdbscan_model = HDBSCAN(min_cluster_size=min_cluster_size, metric='euclidean', prediction_data=True)

    # Create BERTopic model
    topic_model = BERTopic(umap_model=umap_model, 
                           hdbscan_model=hdbscan_model,
                           min_topic_size=min_topic_size, 
                           nr_topics='auto', 
                           vectorizer_model = vectorizer_model, 
                           representation_model=representation_model,
                           n_gram_range = (1,2))

    # Fit the model
    topics, probabilities = topic_model.fit_transform(docs, embeddings)

    # Calculate topic words
    topic_words = {topic_idx: [word for word, _ in topic_model.get_topic(topic_idx)] for topic_idx in set(topics) if topic_model.get_topic(topic_idx)}

    # Calculate coherence score
    coherence_model = CoherenceModel(topics=list(topic_words.values()), texts=tokenized_data, dictionary=dictionary, coherence='c_v')
    coherence_score = coherence_model.get_coherence()

    # Store results with the model
    results.append({'coherence_score': coherence_score,
                    'model': topic_model,
                    'num_topics': len(set(topics)),
                    'n_neighbors': n_neighbors,
                    'min_cluster_size': min_cluster_size,
                    'min_topic_size': min_topic_size})

# Print all models with their coherence scores, number of topics, and parameters
print("All Models:")
for idx, model_result in enumerate(results):
    print(f"\nModel {idx + 1}:")
    print(f"n_neighbors: {model_result['n_neighbors']}")
    print(f"min_cluster_size: {model_result['min_cluster_size']}")
    print(f"min_topic_size: {model_result['min_topic_size']}")
    print(f"Coherence Score: {model_result['coherence_score']}")
    print(f"Number of Topics: {model_result['num_topics']}")

best_model = results[23]['model']
topic_info = best_model.get_topic_info()
topic_info

# Use the 'distribution' strategy to assign outliers to topics
new_topics_dis = best_model.reduce_outliers(docs, topics, strategy="distributions")

copy_model_dis = best_model

copy_model_dis.update_topics(docs, topics=new_topics_dis)

copy_model_dis.get_topic_info()

results = pd.DataFrame(results)
results.to_csv('/content/drive/MyDrive/Thesis Data Science Python/results_coherence.csv')

train_headlines_df['Updated_topics'] = new_topics_dis

# Create a pivot table with all the values from df 
pivot_table = train_headlines_df.pivot_table(index=train_headlines_df.index, columns='Updated_topics', aggfunc='size', fill_value=0)

# Normalize the counts by dividing each cell by the total count of documents for that day
normalized_table_updated = pivot_table.div(pivot_table.sum(axis=1), axis=0)

# fill NA with 0 
normalized_table_updated.fillna(0, inplace=True)

normalized_table_updated.to_csv('/content/drive/MyDrive/Thesis Data Science Python/topic_time_series_train_v6.csv')

"""# Store topic info and visualizations"""

topic_info = copy_model_dis.get_topic_info()
topic_info.to_csv('/content/drive/MyDrive/Thesis Data Science Python/topic_infov6.csv')

copy_model_dis.save("/content/drive/MyDrive/Thesis Data Science Python/topic_modelv6")

embeddings = copy_model_dis.visualize_topics()
plt.savefig("/content/drive/MyDrive/Thesis Data Science Python/embeddings.png")

topic_barchart = copy_model_dis.visualize_barchart()

# Save the figure
plt.savefig("/content/drive/MyDrive/Thesis Data Science Python/topic_barchart.png")

import matplotlib.pyplot as plt


# Visualize term rank decrease
topic_hierarchy = copy_model_dis.visualize_hierarchy()

# Save the figure
plt.savefig("/content/drive/MyDrive/Thesis Data Science Python/topic_hierarchy.png")